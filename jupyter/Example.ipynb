{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic walkthrough\n",
    "\n",
    "Here is a basic walkthrough of how to run the whole pipeline from preprocessing to training the GAN. The walkthrough is only one example to see what one can to to tune and tweek on different datasets, hyperparemeters and changing between models see `Tuning.ipynb`. Every script should be run(```# RUN```) from the repostitory folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "### Datasets\n",
    "The combined dataset with CNN/Daily mail and Exabel data is used. It has 255157 articles with summaries.\n",
    "\n",
    "\n",
    "### Preprocess datasets\n",
    "#### CNN dataset\n",
    "- The tokenized version of the CNN/Daily Mail dataset can be downloaded from [Dataset Link](https://github.com/JafferWilson/Process-Data-of-CNN-DailyMail).\n",
    "- Create a folder `data` in the parent folder of the repo, on the same level as the repo. There is a datafolder in the repo with dummyfiles in the leafnodes you can move.\n",
    "- Place the tokenized data in the `cnn_clean` folder\n",
    "\n",
    "\n",
    "#### Exabel Dataset:\n",
    "- Get from Erlend Aune in json-format\n",
    "- Place in `data` folder\n",
    "\n",
    "**save the exabel data in the same format as CNN/DM**\n",
    "```sh\n",
    "# RUN\n",
    "python3 preprocess/clean_exa.py \n",
    "```\n",
    "\n",
    "```sh\n",
    "# RUN\n",
    "python3 preprocess/combined_preprocess.py\n",
    "```\n",
    "\n",
    "**create vocabulary and preprocess all the articles from both the CNN, daily mail and exabel and save as pickle**\n",
    "```sh\n",
    "# RUN\n",
    "python3 preprocess/preprocess_pointer.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "### Generator pretraining\n",
    "```sh\n",
    "# RUN\n",
    "python3 training/seq2seq/run_experiment.py training/seq2seq/experiments/combined_test_1 0\n",
    "```\n",
    "PS: it prints after every epoch. But logs more frequently in output.log in ```training/seq2seq/experiments/combined_test_1```\n",
    "\n",
    "\n",
    "**time to run: **\n",
    "40h ++\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "### Generate fake data to train discriminator:\n",
    "\n",
    "Creates samples using the generator as fake ones.\n",
    "```sh\n",
    "# RUN\n",
    "python3 evaluation/seq2seq/generate_fake_sampled_data.py 0\n",
    "```\n",
    "\n",
    "- The data is saved in ```../data/cnn_validation_sampled_data```.\n",
    "- Move the data over to the sibling folder: ```cnn_fake_data```.\n",
    "- it is possible to have several fake_data files from different generators. Then evaluate a discriminator on different datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Generate real data to train discriminator:\n",
    "```sh\n",
    "# RUN\n",
    "python3 evaluation/seq2seq/generate_real_data.py\n",
    "```\n",
    "\n",
    "- the real data is then saved in ```../data/cnn_real_data```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Discriminator pretraining\n",
    "- first specify name of real_data_file and fake_data_directory in ```training/classifier/experiments/cnn_test_1/config.json```\n",
    "\n",
    "```sh\n",
    "# RUN\n",
    "python3 training/classifier/run_experiment.py training/classifier/experiments/cnn_test_1 0\n",
    "```\n",
    "\n",
    "- pretrains the Discriminator on the fake data and real data generated\n",
    "- Saves the model in the same folder as the config.json file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### GAN training\n",
    "```sh\n",
    "# RUN\n",
    "python3 training/GAN/run_experiment.py training/GAN/experiments/hb_cnn_test_1 0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Evaluating\n",
    "\n",
    "To evaluate generator models you yourself have to manually copy the models you want evaluated into a fitting ```models_first```-folder\n",
    "\n",
    "#### Evaluate generator\n",
    "To evaluate all the models through all epochs put them in a folder and call this:\n",
    "```sh\n",
    "# RUN\n",
    "python3 evaluation/seq2seq/evaluate_multiple_generators.py output_for_eval/seq2seq/cnn/models_first 0\n",
    "# or\n",
    "python3 evaluation/seq2seq/evaluate_multiple_generators.py output_for_eval/gan/cnn/models_first 0\n",
    "```\n",
    "**time to run:** overnight\n",
    "\n",
    "To calculate rouge for results of all the models:\n",
    "```sh\n",
    "# RUN\n",
    "python3 evaluation/seq2seq/calculate_rouge_in_folder.py output_for_eval/seq2seq/cnn/models_first_eval\n",
    "# or\n",
    "python3 evaluation/seq2seq/calculate_rouge_in_folder.py output_for_eval/gan/cnn/models_first_eval\n",
    "```\n",
    "\n",
    "#### Evaluate Discriminator\n",
    "```sh\n",
    "# RUN\n",
    "python3 evaluation/classifier/test_pretrained_classifier.py 0\n",
    "```\n",
    "\n",
    "Evaluates generater\n",
    "\n",
    "**time to run:** seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "### Tensorboard\n",
    "To see the statistics that are saved in ```log/``` during training.\n",
    "\n",
    "```sh\n",
    " tensorboard --logdir log\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
