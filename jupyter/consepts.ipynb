{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam search\n",
    "Only when testing is beam search used.\n",
    "\n",
    "We choose two parameters: k1 and k2. \n",
    "- (k1)How many beams should be created from the previous beams.  k1 = 3\n",
    "- (k2)How many beams should be kept for each iteration.          k2 = 6\n",
    "\n",
    "The beams are extended with the k1 top scoring words from the decoder output at every expansion. Then it basically averages the log-probabilities of picking the individual words in the sequence. This gives us an average score per beam.\n",
    "\n",
    "(trigram avoidance):\n",
    "repetition avoidance trick in beam search. \n",
    "They force the decoder to never output the same trigram more than once.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roll-out\n",
    "\n",
    "As a requirement to calculate policy gradient(dependant on rewardfunction ex:Rouge) to optimize the generator\n",
    "- REINFORCE algorihtm is dependant on reward.\n",
    "- discriminator requires a complete sequence as input, as the reward will not be accurate in the case of a partial sequence.\n",
    "\n",
    "**Option 1: Naive roll-out**\n",
    "- Generate the entire sequence\n",
    "- Accumulate the log probability of producing each token\n",
    "- Give one single reward to the entire sequence\n",
    "- Which then will be multiplied to each log probability when doing gradient ascents\n",
    "- fast\n",
    "\n",
    "**Option 2: Monte-Carlo roll-out**\n",
    "- Instead of evaluating the entire sequence, give different reward to each time-step(wordguess)\n",
    "- N-time Monte-Carlo roll-out for each intermediate time-step, providing fine-grained rewards for each token (action) sampled by\n",
    "the generator.\n",
    "- slow\n",
    "\n",
    "How to determine next token?\n",
    "- **Sampling**: Sample the words as a distribution, where the distribution is the probabilities provided by the decoderâ€™s output.\n",
    "- **Argmax**: Choose the word from the decoder output with the highest probability.\n",
    "\n",
    "### Baseline\n",
    "goal: moderate, reduce noise of training, stabelize, make rewardfunction more smooth/stable\n",
    "\n",
    "Rolling avg vs argmax\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforce algorithm\n",
    "- expectation of the sample gradient is *proportional* to the actual gradient of the performancemeasure as a function of the parameter.\n",
    "- estimates the policy/policy gradient basically\n",
    "- uses all the future rewards up until the end of the episode.\n",
    "- all updates are made in retrospect after the episode is completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "\n",
    "Rolling avg vs argmax\n",
    "\n",
    "basically make reward function more smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
