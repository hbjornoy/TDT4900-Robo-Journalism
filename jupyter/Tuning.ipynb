{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning of models and functions\n",
    "Here is a little appendix too know where to tune the different parts of the different models. This is not an exhaustive list but hopefully the most important consepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "### Datasets\n",
    "\n",
    "#### CNN dataset\n",
    "- 307 525 article-summary pairs. \n",
    "- 532 449 distinct tokens\n",
    "- keep vocabulary of 50 000 tokens\n",
    "- after preprocessing(removing too short articles and abstracts): 173802 article-summary pairs \n",
    "\n",
    "#### Exabel dataset\n",
    "- 361 319 article-summary pairs. \n",
    "- Keep vocabulary of 50 000 tokens\n",
    "- After preprocessing: 173802 article-summary pairs \n",
    "\n",
    "### Clean/Preprocess datasets\n",
    "- `clean_exa.py` has not incorporated any cleaning of exabel data, it basically transfers data from json structure into single .story files per article summarypair into `exa_clean` folder. It is possible to do more cleaning in this step.\n",
    "- `preprocess_XXX.py` files only accumelate the data from single .story files to seperate too two article and summary files.\n",
    "    - One can decide what range token-length an article and summary should have by altering `max_article_tokens`, `min_abstract_tokens` etc in lines:{12,15}\n",
    "- `preprocess_pointer.py` creates vocabulary and saves in pickle-format which is used in the other segments too retrieve summary-article pairs and vocabulary\n",
    "    - Set path variable of `relative_path`(where to load preprocessed data) and `save_path_dataset`(where to save pickle) lines:{153,158}\n",
    "    - Set `limit` (vocabulary size) on line:165   \n",
    "\n",
    "### Pretraining of Generator\n",
    "- Most parameters can be set in the `config.json` file that you give as input.\n",
    "- Stuff that must be changed in code:\n",
    "    - Change decoder in `training/seq2seq/run_experiment.py` from PointerGeneratorDecoder <-> AttnDecoderRNN line:130\n",
    "    \n",
    "### Generating of fake data for dicriminator\n",
    "- The data is saved in ```../data/cnn_validation_sampled_data```.\n",
    "- it is possible to have several fake_data files from different generators. Then evaluate a discriminator on different datasets. (see testing pretrained discriminator)\n",
    "\n",
    "### Generating of real data for dicriminator\n",
    "- the real data is then saved in ../data/cnn_real_data\n",
    "\n",
    "### Pretraining of discriminator\n",
    "- First specify name of real_data_file and fake_data_directory in training/classifier/experiments/cnn_test_1/config.json\n",
    "- Most parameters can be set in the `config.json` file that you give as input.\n",
    "- Change how often to save model in `training/classifier/train.py` line:121\n",
    "\n",
    "### GAN training\n",
    "- Most parameters are set in the `config.json` file that you give as input.\n",
    "- Alternating Rollout strategy change `generator` in run `training/GAN/run_experiment.py`, line:193-206\n",
    "    - GeneratorRlStrat() - Naive roll-out strategy\n",
    "    - GeneratorSeqGanStrat() - Monte carlo roll-out strategy\n",
    "- Set RL Objective function by choosing `discriminator` in run `training/GAN/run_experiment.py`, line:208-216\n",
    "    - GANDiscriminator()\n",
    "    - RougeDiscriminator() or\n",
    "    - JointRougeAndGANDiscriminator() - which is a mix of the two. The input `phi`(lamdba in config.json..) is a soft switch and decides how much Rouge is weigthed where phi=1 is equivalent to RougeDiscriminator()\n",
    "- beta in the config.json sets the linear combination of RL-objective and MLE, where beta=1 is exclusively RL-objective\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
