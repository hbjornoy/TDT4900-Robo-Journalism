{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic walkthrough\n",
    "\n",
    "**//////////////////////  DAILY TODO  //////////////////////**\n",
    "- make GAN work, fix discriminator\n",
    "\n",
    "**//////////////////////  GENERAL TODO  //////////////////////**\n",
    "- Better more dynamic way to store logs\n",
    "- Better more dynamic way to store models/config\n",
    "- Define config and run script through functioncall. Produce new folder with relevant: log, model, results etc\n",
    "- Reproduce results in report\n",
    "\n",
    "- apply pretrained model on different data and analyse results\n",
    "\n",
    "- train on different data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "### Datasets\n",
    "\n",
    "#### CNN dataset\n",
    "307 525 article-summary pairs. \n",
    "532 449 distinct tokens\n",
    "keep vocabulary of 50 000 tokens\n",
    "Split: 294 525 training pairs, 2000 validation pairs(small val-set because beam-search is expensive) and 11 000 (∼ 3.6%) test pairs.\n",
    "\n",
    "for discriminator pretraining, it only uses 160768 real for training, 13024 fake and real for test (total: 26058)\n",
    "\n",
    "#### Fake/sampled dataset from generator\n",
    "fake/sampled size = 160768\n",
    "real size         = 173802\n",
    "\n",
    "\n",
    "### Preprocess datasets\n",
    "#### CNN dataset\n",
    "- The tokenized version of the CNN/Daily Mail dataset can be downloaded from [Dataset Link](https://github.com/JafferWilson/Process-Data-of-CNN-DailyMail).\n",
    "- Place the folder with...\n",
    "- Run...\n",
    "```sh\n",
    "code here\n",
    "```\n",
    "- Then run...\n",
    "```sh\n",
    "code here\n",
    "```\n",
    "\n",
    "#### Exabel dataset\n",
    "- First clean the json-file with by running:\n",
    "```sh\n",
    "code here\n",
    "```\n",
    "- Then preprosess it by\n",
    "```sh\n",
    "code here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "### Generator pretraining\n",
    "```sh\n",
    "python3 training/GAN/run_experiment.py training/GAN/experiments/hb_cnn_test_1 0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**//////////////////////  TODO  //////////////////////**\n",
    "- Fix config params ALL the model parameters\n",
    "- Run the model that they ran in the report\n",
    "- Define decoder_hidden, implicit defined in code, read up\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **Report**\n",
    "\n",
    "*list the hyperparameters*\n",
    "```sh\n",
    "Hyperparameter         value\n",
    "\n",
    "Batch size             16\n",
    "Learning rate          0.015 or 0.001  (they talked about lowering to 0.0001 after pretraining and increasing batch_size to 50)\n",
    "L2 penalty(ADAgrad)    10−5\n",
    "Embedding size         100\n",
    "Encoder hidden size    128\n",
    "Decoder hidden size    256\n",
    "Teacher forcing ratio  1.0\n",
    "```\n",
    "*Results*\n",
    "```sh\n",
    "                             ROUGE-1  ROUGE-2  ROUGE-L\n",
    "Pretrained model (Epoch 13)  0.37631  0.16423  0.34104\n",
    "```\n",
    "\n",
    "#### **Exabel**\n",
    "*My hyperparameters*\n",
    "```sh\n",
    "Hyperparameter         value\n",
    "\n",
    "fill in\n",
    "```\n",
    "\n",
    "\n",
    "**time to run: **\n",
    "40h 32min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "### Generate fake data to train discriminator:\n",
    "```sh\n",
    "python3 evaluation/seq2seq/generate_fake_sampled_data.py 0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- creates samples from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Generate real data to train discriminator:\n",
    "```sh\n",
    "python3 evaluation/seq2seq/generate_fake_sampled_data.py 0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Discriminator pretraining\n",
    "```sh\n",
    "python3 training/classifier/run_experiment.py training/classifier/experiments/cnn_test_1 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- creates samples from x nr of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Report**\n",
    "\n",
    "```sh\n",
    "This includes using\n",
    "filter windows of 3, 4 and 5 \n",
    "with 100 feature maps each, \n",
    "embedding size of 100 (trained from scratch), and \n",
    "dropout set to 0.5. \n",
    "Adam is our chosen optimizer and we use it with 0.0001 learningrate, \n",
    "a 10−5 L2 penalty and a batch size of 64.\n",
    "```\n",
    "\n",
    "\n",
    "#### **Exabel**\n",
    "```sh\n",
    "{\n",
    "  \"train\": {\n",
    "    \"real_data_file\": \"../../data/cnn_real_data/cnn_real_1\",\n",
    "    \"fake_data_directory\": \"../../data/cnn_sampled_data/\",\n",
    "    \"validation_data_directory\": \"../../data/cnn_validation_sampled_data/\",\n",
    "    \"vocabulary_path\": \"../../data/cnn_pickled/cnn_pointer_50k\",\n",
    "    \"num_articles\": -1,\n",
    "    \"num_evaluate\": 13000,\n",
    "    \"num_epochs\": 10,\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.0001\n",
    "  },\n",
    "  \"model\": {\n",
    "    \"hidden_size\": 128,\n",
    "    \"dropout_p\": 0.5,\n",
    "    \"num_kernels\": 100,\n",
    "    \"kernel_sizes\": [\n",
    "      3,\n",
    "      4,\n",
    "      5\n",
    "    ]\n",
    "  },\n",
    "  \"save\": {\n",
    "    \"save_file\": \"cnn_classifier_sampled_folder_test5.tar\",\n",
    "    \"save_after_dataset_num\": 10\n",
    "  },\n",
    "  \"log\": {\n",
    "    \"print_every\": 1000\n",
    "  },\n",
    "  \"tensorboard\": {\n",
    "    \"log_path\": \"../../log/classifier/cnn_test_1\"\n",
    "  },\n",
    "  \"experiment_path\": \"training/classifier/experiments/cnn_test_1\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### GAN training\n",
    "```sh\n",
    "python3 training/GAN/run_experiment.py training/GAN/experiments/hb_cnn_test_1 0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**//////////////////////  TODO  //////////////////////**\n",
    "- Fix config params for naive rollout strategy\n",
    "- Fix config params for objective function for discriminator\n",
    "- Fix config params for other strategies\n",
    "- fasdf\n",
    "- sdfsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run generator pretraining, training only with MLE (NLLLoss) takes almost 2 days\n",
    "\n",
    "# list the hyperparameters in the report\n",
    "\n",
    "# My hyperparameters\n",
    "\n",
    "# time\n",
    "\n",
    "# results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Evaluating\n",
    "\n",
    "#### Evaluate generator\n",
    "To evaluate all the models through all epochs put the in a folder and call this:\n",
    "```sh\n",
    "python3 evaluation/seq2seq/evaluate_multiple_generators.py output_for_eval/seq2seq/cnn/models_first 0\n",
    "# or\n",
    "python3 evaluation/seq2seq/evaluate_multiple_generators.py output_for_eval/gan/cnn/models_first 0\n",
    "```\n",
    "**time to run:** takes some time :|  class:overnight maybe for >10 models\n",
    "\n",
    "To calculate rouge for results of all the models:\n",
    "```sh\n",
    "python3 evaluation/seq2seq/calculate_rouge_in_folder.py output_for_eval/seq2seq/cnn/models_first_eval\n",
    "# or\n",
    "python3 evaluation/seq2seq/calculate_rouge_in_folder.py output_for_eval/gan/cnn/models_first_eval\n",
    "```\n",
    "\n",
    "#### Evaluate Discriminator\n",
    "```sh\n",
    "python3 evaluation/classifier/test_pretrained_classifier.py 0\n",
    "```\n",
    "\n",
    "Evaluates generater\n",
    "\n",
    "**time to run:** seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
